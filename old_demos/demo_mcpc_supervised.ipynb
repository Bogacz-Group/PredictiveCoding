{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import predictive_coding as pc\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class contains the parameters of the prior mean \\mu parameter (see figure)\n",
    "class BiasLayer(nn.Module):\n",
    "    def __init__(self, num_features, offset=0.):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(offset * torch.ones(num_features))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.zeros_like(x) + self.bias\n",
    "\n",
    "# function to add noise to the inference dynamics of the PC layers\n",
    "def random_step(t, _trainer, var=2.):\n",
    "    \"\"\"var: sets the variance of the noise.\n",
    "    \"\"\"\n",
    "    xs = _trainer.get_model_xs()\n",
    "    optimizer = _trainer.get_optimizer_x()\n",
    "    for x in xs:\n",
    "        x.grad.normal_(0.,np.sqrt(var/optimizer.defaults['lr']))\n",
    "    optimizer.step()\n",
    "\n",
    "def loss_fn(output, _target):\n",
    "    return (output - _target).pow(2).sum() * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function\n",
    "def test(model, loader):\n",
    "    test_model = nn.Sequential(\n",
    "        BiasLayer(options.layer_sizes[0], offset=0.1),\n",
    "        pc.PCLayer(),\n",
    "        model\n",
    "    ).to(device)\n",
    "\n",
    "    test_model.train()\n",
    "\n",
    "    test_trainer = pc.PCTrainer(\n",
    "        test_model, \n",
    "        T=200,\n",
    "        update_x_at='all',\n",
    "        optimizer_x_fn=optim.SGD,\n",
    "        optimizer_x_kwargs={\"lr\": 0.1},\n",
    "        update_p_at='never', # do not update parameters during inference\n",
    "        plot_progress_at=[],\n",
    "        x_lr_discount=0.5,\n",
    "    )\n",
    "    \n",
    "    correct_cnt = 0\n",
    "    for data, target in loader:\n",
    "        data = data.to(device)\n",
    "        target = F.one_hot(target, num_classes=10).to(torch.float32).to(device)\n",
    "        results = test_trainer.train_on_batch(\n",
    "            inputs=target,\n",
    "            loss_fn=loss_fn,\n",
    "            loss_fn_kwargs={'_target': data},\n",
    "            is_log_progress=False,\n",
    "            is_return_results_every_t=False,\n",
    "        )\n",
    "        pred = test_model[1].get_x()\n",
    "        correct = pred.argmax(dim=1).eq(target.argmax(dim=1)).sum().item()\n",
    "        correct_cnt += correct\n",
    "    return correct_cnt / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Options:\n",
    "    pass\n",
    "options = Options()\n",
    "\n",
    "options.batch_size = 500\n",
    "options.train_size = 10000\n",
    "options.test_size = 1000\n",
    "options.layer_sizes = [10, 256, 256, 784]\n",
    "options.activation = nn.Tanh()\n",
    "\n",
    "def get_mnist(options):\n",
    "    transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                    transforms.Lambda(lambda x: torch.flatten(x))])\n",
    "    train = datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "    test = datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
    "    \n",
    "    if options.train_size != len(train):\n",
    "        train = torch.utils.data.Subset(train, random.sample(range(len(train)), options.train_size))\n",
    "    if options.test_size != len(test):\n",
    "        test = torch.utils.data.Subset(test, random.sample(range(len(test)), options.test_size))\n",
    "\n",
    "    # Split the training set into training and validation sets\n",
    "    train_len = int(len(train) * 0.9)  # 80% of data for training\n",
    "    val_len = len(train) - train_len   # remaining 20% for validation\n",
    "    train_set, val_set = torch.utils.data.random_split(train, [train_len, val_len])\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=options.batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_set, batch_size=options.batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=options.batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_mixing = 100\n",
    "T_sampling = 100\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(options.layer_sizes[0], options.layer_sizes[1]), # generates the prediction\n",
    "    pc.PCLayer(),\n",
    "    options.activation,\n",
    "    nn.Linear(options.layer_sizes[1], options.layer_sizes[2]),\n",
    "    pc.PCLayer(),\n",
    "    options.activation,\n",
    "    nn.Linear(options.layer_sizes[2], options.layer_sizes[3]),\n",
    ").to(device)\n",
    "\n",
    "# sample a batch of data\n",
    "train_loader, val_loader, test_loader = get_mnist(options)\n",
    "\n",
    "model.train()\n",
    "\n",
    "# warm-up inference for mcpc\n",
    "inference_trainer = pc.PCTrainer(\n",
    "    model, \n",
    "    T = T_mixing+T_sampling, \n",
    "    update_x_at = 'all', \n",
    "    optimizer_x_fn = optim.Adam,\n",
    "    optimizer_x_kwargs = {'lr': 0.1},\n",
    "    update_p_at = 'never',   \n",
    "    plot_progress_at = [],\n",
    ")\n",
    "\n",
    "# training the model\n",
    "train_trainer = pc.PCTrainer(\n",
    "    model, \n",
    "    T = T_mixing+T_sampling, \n",
    "    update_x_at = 'all', \n",
    "    optimizer_x_fn = optim.Adam,\n",
    "    optimizer_x_kwargs = {'lr': 0.1},\n",
    "    update_p_at = 'last',   \n",
    "    accumulate_p_at=[i + T_mixing for i in range(T_sampling)],\n",
    "    optimizer_p_fn = optim.Adam,\n",
    "    optimizer_p_kwargs = {\"lr\": 0.001, \"weight_decay\":0.001},\n",
    "    plot_progress_at= [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Projects\\PredictiveCoding\\predictive_coding\\utils.py:9: RuntimeWarning: In PCTrainer.train_on_batch, you have is_checking_after_callback_after_t enabled, this will slow down training. Set to False to disable it. \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.2871111111111111\n",
      "validation accuracy: 0.252\n",
      "epoch 1\n",
      "training accuracy: 0.45111111111111113\n",
      "validation accuracy: 0.436\n",
      "epoch 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m     pc_results \u001b[38;5;241m=\u001b[39m inference_trainer\u001b[38;5;241m.\u001b[39mtrain_on_batch(\n\u001b[0;32m     10\u001b[0m         inputs\u001b[38;5;241m=\u001b[39mtarget, \n\u001b[0;32m     11\u001b[0m         loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m         is_checking_after_callback_after_t\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     )\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# mc inference\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     mc_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_target\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback_after_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback_after_t_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_trainer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_trainer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_sample_x_at_batch_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_log_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_return_results_every_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_checking_after_callback_after_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest(model,\u001b[38;5;250m \u001b[39mtrain_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest(model,\u001b[38;5;250m \u001b[39mval_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Projects\\PredictiveCoding\\predictive_coding\\pc_trainer.py:767\u001b[0m, in \u001b[0;36mPCTrainer.train_on_batch\u001b[1;34m(self, inputs, loss_fn, loss_fn_kwargs, is_sample_x_at_batch_start, is_reset_optimizer_x_at_batch_start, is_reset_optimizer_p_at_batch_start, is_unwrap_inputs, is_optimize_inputs, callback_after_backward, callback_after_backward_kwargs, callback_after_t, callback_after_t_kwargs, is_log_progress, is_return_results_every_t, is_checking_after_callback_after_t, debug, backward_kwargs, is_clear_energy_after_use, is_return_outputs, is_return_representations, is_return_xs, is_return_batchelement_loss)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unwrap_with \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 767\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m unwrap_with \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    769\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs)\u001b[38;5;241m.\u001b[39mclone()\n",
      "File \u001b[1;32mc:\\Users\\mufen\\miniconda3\\envs\\dlenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mufen\\miniconda3\\envs\\dlenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\mufen\\miniconda3\\envs\\dlenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Projects\\PredictiveCoding\\predictive_coding\\pc_layer.py:275\u001b[0m, in \u001b[0;36mPCLayer.forward\u001b[1;34m(self, mu, energy_fn_additional_inputs)\u001b[0m\n\u001b[0;32m    269\u001b[0m energy_fn_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m'\u001b[39m: mu,\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: x,\n\u001b[0;32m    272\u001b[0m }\n\u001b[0;32m    273\u001b[0m energy_fn_inputs\u001b[38;5;241m.\u001b[39mupdate(energy_fn_additional_inputs)\n\u001b[1;32m--> 275\u001b[0m energy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_energy_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menergy_fn_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_energy_fn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_S \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# [batch_size, size_mu, size_x]\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     energy \u001b[38;5;241m=\u001b[39m energy \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_S\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Projects\\PredictiveCoding\\predictive_coding\\pc_layer.py:17\u001b[0m, in \u001b[0;36mPCLayer.<lambda>\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPCLayer\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"PCLayer.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m        PCLayer should be inserted between layers where you want the error to be propagated\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m            in the predictive coding's (PC's) way, instead of the backpropagation's (BP's) way.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     16\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m---> 17\u001b[0m         energy_fn: typing\u001b[38;5;241m.\u001b[39mCallable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m inputs: \u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m,\n\u001b[0;32m     19\u001b[0m         energy_fn_kwargs: \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m {},\n\u001b[0;32m     20\u001b[0m         sample_x_fn: typing\u001b[38;5;241m.\u001b[39mCallable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m inputs: inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmu\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach(\n\u001b[0;32m     21\u001b[0m                 )\u001b[38;5;241m.\u001b[39mclone(),\n\u001b[0;32m     22\u001b[0m         S: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m         M: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m         is_holding_error: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m         is_keep_energy_per_datapoint: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m     ):\n\u001b[0;32m     27\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new instance of ``PCLayer``.\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m        Behavior of pc_layer:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m            is_keep_energy_per_datapoint: if keep energy per datapoint (can get via self.energy_per_datapoint()).\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f'epoch {i}')\n",
    "    correct_cnt = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = F.one_hot(target, num_classes=10).to(torch.float32).to(device)\n",
    "\n",
    "        # initialise sampling\n",
    "        pc_results = inference_trainer.train_on_batch(\n",
    "            inputs=target, \n",
    "            loss_fn=loss_fn,\n",
    "            loss_fn_kwargs={'_target':data},\n",
    "            is_log_progress=False,\n",
    "            is_return_results_every_t=False,\n",
    "            is_checking_after_callback_after_t=False\n",
    "        )\n",
    "        # mc inference\n",
    "        mc_results = train_trainer.train_on_batch(\n",
    "            inputs=target,\n",
    "            loss_fn=loss_fn,\n",
    "            loss_fn_kwargs={'_target': data}, \n",
    "            callback_after_t=random_step, \n",
    "            callback_after_t_kwargs={'_trainer': train_trainer},\n",
    "            is_sample_x_at_batch_start=False,\n",
    "            is_log_progress=False,\n",
    "            is_return_results_every_t=False,\n",
    "            is_checking_after_callback_after_t=False\n",
    "        )\n",
    "\n",
    "    print(f'training accuracy: {test(model, train_loader)}')\n",
    "    print(f'validation accuracy: {test(model, val_loader)}')\n",
    "\n",
    "print(f'test accuracy: {test(model, test_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
